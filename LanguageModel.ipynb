{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Character-level language model\n",
    "\n",
    "In this notebook, we train a language model on a small corpus and use it to generate new text in the same style. We are defining the conditional probability of the next character in a sequence given characters up until the current time. To do this we will use an LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# replace with your own root directory\n",
    "ROOT=\"./\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert characters from the text corpus into tokens from a fixed *vocabulary*.  Convert upper case letters to lower case. Characters other than letters, full-stop and space will be omitted (e.g. other punctuation, mumerical digits).  We will then use a one-hot encoding for each token as input to the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character encoding: lower case letters, full-stop and space.\n",
    "chars = 'abcdefghijklmnopqrstuvwxyz. '\n",
    "input_size = len(chars)  # for one-hot encoding\n",
    "\n",
    "# define a mapping from char to integer (token index) and vice versa\n",
    "char_to_int = {}    # ensure the dictionaries are empty\n",
    "int_to_char = {}\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "def tokenize(data):\n",
    "    # tokenize text string\n",
    "    \n",
    "    # convert to lower case\n",
    "    data = data.lower()\n",
    "    \n",
    "    # integer encode (tokenize) each character; -1 if not in chars, drop these\n",
    "    idx = torch.tensor([char_to_int.get(char,-1) for char in data], dtype=torch.long)\n",
    "    idx = idx[idx != -1]  \n",
    "    return idx\n",
    "\n",
    "def onehot(idx, input_size):\n",
    "    # convert to one-hot form (batch_size x sequence_length x input_size).\n",
    "    # works for any tensor size and adds one-hot encoding of indices as a new dimension at the end\n",
    "    inputs = torch.eye(input_size,dtype=int)[idx]      \n",
    "    inputs = inputs.float()\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the PyTorch data loader facilities, we need to define a new dataset to access the text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alice(Dataset):\n",
    "    \"\"\" Alice in Wonderland custom dataset \"\"\"\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        # read in the text and tokenize\n",
    "        f = open(ROOT + 'alice_in_wonderland.txt')\n",
    "        data = f.read()\n",
    "        self.idx = tokenize(data)     \n",
    "        self.size = size    # the size of text windows to be used in training\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Returns one data item (token sequence of the given size) \"\"\"\n",
    "        return self.idx[index:(index+self.size)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.idx) - self.size\n",
    "    \n",
    "dataset = Alice(50)\n",
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network. This is an LSTM followed by a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 28])\n",
      "torch.Size([512, 128])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([28, 128])\n",
      "torch.Size([28])\n"
     ]
    }
   ],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MyRNN, self).__init__()\n",
    "        \n",
    "        # single LSTM layer, with dropout(0.5)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(128,input_size)\n",
    "        \n",
    "    def forward(self, input, zero_hidden = True):\n",
    "        if zero_hidden:\n",
    "            # hidden state defaults to zero\n",
    "            output, hiddens = self.lstm(input)    \n",
    "        else:\n",
    "            # use the hidden state stored from previous invocation\n",
    "            output, hiddens = self.lstm(input, self.hiddens)    \n",
    "        \n",
    "        # update stored hidden state for future invocation\n",
    "        self.hiddens = hiddens\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "                    \n",
    "net = MyRNN(input_size) \n",
    "\n",
    "for param in net.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network. Each window of text is trimmed by one token at the start for input and one token at the end for the target. This trains the network to predict the next character from all previous characters in the window.\n",
    "\n",
    "`nn.LSTM` will roll-out the LSTM to operate on the given sequence. We don't need to do this explicitly. Likewise, `nn.Linear` operates independently on each time step in the time dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 50\n",
    "results_path = ROOT+'epochs50hidden128adam.pt'\n",
    "losses = np.zeros(nepochs)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "net.train()    # to be sure we are in training mode\n",
    "for epoch in range(nepochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    n = 0\n",
    "    for i, idx in enumerate(train_dataloader, 0):\n",
    "        \n",
    "        inputs = onehot(idx, input_size)\n",
    "        \n",
    "        # trim to advance target by one timestep whilst keeping input and target sequences the same length\n",
    "        inputs = inputs[:,:-1,:]\n",
    "        targets = idx[:,1:]\n",
    "        \n",
    "         # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward, backward, and update parameters\n",
    "        outputs = net(inputs)    # initial hidden state will default to zero\n",
    "        loss = loss_fn(outputs.transpose(1,2), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # accumulate loss\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "       \n",
    "    losses[epoch] = meanloss = running_loss / n\n",
    "    print(f\"epoch: {epoch+1} loss: {meanloss : .3f}\")\n",
    "    \n",
    "# save network parameters and losses\n",
    "torch.save({\"state_dict\": net.state_dict(), \"losses\": losses}, results_path)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate novel text primed by the given prior text.\n",
    "\n",
    "Run the model repeatedly on the currently generated text, starting with the given prior text.\n",
    "Select the maximum probability token as the next token in the sequence.\n",
    "\n",
    "In subsequent runs of the model, only input the most recently selected token since we use the final hidden state and cell state from the previous run to provide the history. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The india what a little party eneandly  the rabbit said alice said the caterpillar and it was the little golden key and the hatter was a little bit she said to herself in and the mock turtle she things was that\n"
     ]
    }
   ],
   "source": [
    "# restore model parameters and losses\n",
    "results_path = ROOT+'epochs50hidden128adam.pt'\n",
    "data = torch.load(results_path)\n",
    "net.load_state_dict(data[\"state_dict\"])\n",
    "\n",
    "# set prior text\n",
    "pretext = 'The india '\n",
    "idx = tokenize(pretext)    # initialise token sequence and generated sequence\n",
    "gen_length = 200    # length of generated text\n",
    "output = pretext    # initialise output text\n",
    "\n",
    "# sample from the network.\n",
    "zero_hidden = True    # initialise hidden state to zero for the first time through\n",
    "net.eval()    # set eval mode so that dropout is omitted by the module\n",
    "with torch.no_grad():    # turn off gradient calculations for the evaluation\n",
    "    for i in range(gen_length):\n",
    "        \n",
    "        inputs = onehot(idx, input_size)\n",
    "        inputs = torch.unsqueeze(inputs,0) # add batch dimension of size 1: 1 x len(idx) x input_size\n",
    "        \n",
    "        outputs = net(inputs, zero_hidden)\n",
    "        _, predicted = torch.max(outputs, 2)\n",
    "        \n",
    "        token = predicted[0,-1].item()    # extract final predicted token as input for next timestep\n",
    "        output = output + int_to_char[token]       # decode token and append to output\n",
    "        idx = torch.tensor([token])    # input sequence consisting of a single time step\n",
    "        zero_hidden = False               # pass on the hidden state in future iterations\n",
    "          \n",
    "print(output)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the mean loss after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# restore model parameters and losses\n",
    "results_path = ROOT+'epochs50hidden128adam.pt'\n",
    "data = torch.load(results_path)\n",
    "losses = data[\"losses\"]\n",
    "\n",
    "plt.plot(losses, 'r')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training loss on Alice in Wonderland dataset' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional exercises\n",
    "1. Retrain the model using a vanilla RNN and see what happens to the quality of the language model.\n",
    "2. Add a second LSTM layer and train on a larger body of text. Does this improve the quality of the generated text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
